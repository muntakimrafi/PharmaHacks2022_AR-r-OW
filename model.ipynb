{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789f86ed",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1380b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245e63b",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21709a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Samplenormalized_train_data.csv\", index_col=False)\n",
    "X = df.iloc[:, 2:1096]\n",
    "Y = df.iloc[:, 1096]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42e0f8",
   "metadata": {},
   "source": [
    "# use weighting to provide higher weight to class with fewer samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b618c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [len(Y) / len(Y[Y == 'Healthy']), len(Y) / len(Y[Y == 'Disease-1']), len(Y) / len(Y[Y == 'Disease-2']),\n",
    "                                                   len(Y) / len(Y[Y == 'Disease-3'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edae31",
   "metadata": {},
   "source": [
    "# assign labels to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be77ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[Y == 'Healthy'] = 0\n",
    "Y[Y == 'Disease-1'] = 1\n",
    "Y[Y == 'Disease-2'] = 2\n",
    "Y[Y == 'Disease-3'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2551c1",
   "metadata": {},
   "source": [
    "# prepare data for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87a98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#create data matrix\n",
    "data_matrix = xgb.DMatrix(data = X, label = Y)\n",
    "\n",
    "# create train and validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "# the weight for each class needs to be assigned to the samples individually in xgboost. So we create a weight matrix\n",
    "#that is the same length as samples in the train data\n",
    "convert_to_weight = {'0': w[0], '1': w[1], '2':w[2]*10, '3': w[3]}\n",
    "weight_matrix = [convert_to_weight[str(i)] for i in y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db9175",
   "metadata": {},
   "source": [
    "# hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9ea761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'n_estimators': [50, 100, 150, 200],\n",
    "#         'max_depth': [6, 8, 10],\n",
    "#         'learning_rate': [0.05, 0.1, 0.2],\n",
    "#         'gamma': [0, 0.5, 1],\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'subsample': [0.6, 0.8, 1],\n",
    "#         'colsample_by_tree': [0.6, 0.8, 1]}\n",
    "\n",
    "# xgb = XGBClassifier(objective='multi:softprob', silent=True, nthread=6)\n",
    "# evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "\n",
    "# grid_search = GridSearchCV(estimator = xgb, param_grid=params, verbose=3, scoring = 'f1_weighted')\n",
    "# grid_search.fit(X_train, y_train, eval_set=evaluation, eval_metric=\"mlogloss\", early_stopping_rounds=10, sample_weight = weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85921c6b",
   "metadata": {},
   "source": [
    "Point to be noted, we could not select the complete hyperparameter space we wanted to explore. Therefore, we limited it to manual search of the features we thought was significant enough and seleced the best performing one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4779090",
   "metadata": {},
   "source": [
    "# hyperparameter selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eebbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected parameters\n",
    "params = {\n",
    "            'objective':'multi:softprob',\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators':200,\n",
    "            'n_threads' : 8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'alpha': 10\n",
    "        }\n",
    "           \n",
    "# initialize the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "#initialize data\n",
    "evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "\n",
    "#fit the model\n",
    "xgb_clf.fit(X_train, y_train, eval_set=evaluation, eval_metric=\"mlogloss\", early_stopping_rounds=100, sample_weight = weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a7c8e",
   "metadata": {},
   "source": [
    "# check performance in validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0787393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score  0.8080197601948824\n",
      "per classs f1 score  [0.84380306 0.86212361 0.33160622 0.83673469]\n",
      "confusion matrix \n",
      " [[497  20  64   3]\n",
      " [ 21 272   3  18]\n",
      " [ 58   2  32   0]\n",
      " [ 18  23   2 164]]\n",
      "cohen cappa score  0.7041888051402446\n"
     ]
    }
   ],
   "source": [
    "#make prediction\n",
    "y_pred = xgb_clf.predict(X_val)\n",
    "\n",
    "print('weighted f1 score ', f1_score(list(y_val), list(y_pred), average = 'weighted'))\n",
    "print('per classs f1 score ', f1_score(list(y_val), list(y_pred), average = None))\n",
    "print('confusion matrix \\n', confusion_matrix(list(y_val), list(y_pred)))\n",
    "print('cohen cappa score ', cohen_kappa_score(list(y_val), list(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e05d4",
   "metadata": {},
   "source": [
    "# save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffd8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_final.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_clf, 'model_final.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce764c",
   "metadata": {},
   "source": [
    "# bacterias which are informative for the prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51f8bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n"
     ]
    }
   ],
   "source": [
    "feature_important = xgb_clf.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965441b",
   "metadata": {},
   "source": [
    "443 bacterias were deemed important by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02395bcc",
   "metadata": {},
   "source": [
    "# test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "389e156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and process the test data the same way as train data\n",
    "\n",
    "df = pd.read_csv(\"Samplenormalized_test_data.csv\", index_col=False)\n",
    "X_test = df.iloc[:, 2:1096]\n",
    "Y_test = df.iloc[:, 1096]\n",
    "\n",
    "Y_test[Y_test == 'Healthy'] = 0\n",
    "Y_test[Y_test == 'Disease-1'] = 1\n",
    "Y_test[Y_test == 'Disease-2'] = 2\n",
    "Y_test[Y_test == 'Disease-3'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79278931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score  0.7986608192335989\n",
      "per classs f1 score  [0.84245221 0.84387097 0.38135593 0.7887931 ]\n",
      "confusion matrix \n",
      " [[639  23  72  14]\n",
      " [ 37 327   2  24]\n",
      " [ 65   5  45   0]\n",
      " [ 28  30   2 183]]\n",
      "cohen cappa score  0.6873489657005085\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"model_file_name.pkl\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "print('weighted f1 score ', f1_score(list(Y_test), list(y_pred), average = 'weighted'))\n",
    "print('per classs f1 score ', f1_score(list(Y_test), list(y_pred), average = None))\n",
    "print('confusion matrix \\n', confusion_matrix(list(Y_test), list(y_pred)))\n",
    "print('cohen cappa score ', cohen_kappa_score(list(Y_test), list(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7345b",
   "metadata": {},
   "source": [
    "# differentially expressed bacteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ed69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv file created in R. need some string formating as R doesn't read hyphens\n",
    "diff_bacts = pd.read_csv('DEB_strictfiltered_training.csv')\n",
    "diff_bacts = list(diff_bacts.iloc[:,0])\n",
    "diff_bacts = ['Bacteria.' + bact.split('Bacteria.')[1] for bact in diff_bacts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3ae4a",
   "metadata": {},
   "source": [
    "### compare the differenetially enriched bacteria and bacteria found to be important by our xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1fdb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for bact in diff_bacts:\n",
    "    if bact not in keys:\n",
    "        count += 1\n",
    "        print('found a enriched bacteria that is not identified by the model')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fcc26",
   "metadata": {},
   "source": [
    "So all the bacteria found to be important by xgboost are enriched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd06279",
   "metadata": {},
   "source": [
    "# What if we only use the enriched bacteria to train the model (6.5% data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8229930",
   "metadata": {},
   "source": [
    "Now we see whether we can just use this statistically significant bacteria to train the model. The motivation is that if we find the bacteria which are required to classify between different classes it would reduce our screening space. Also it would reduce the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94624ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Samplenormalized_train_data.csv\", index_col=False)\n",
    "X = df[diff_bacts]\n",
    "Y = df.iloc[:, 1096]\n",
    "\n",
    "# the weight for each class needs to be assigned to the samples individually in xgboost. So we create a weight matrix\n",
    "#that is the same length as samples in the train data\n",
    "convert_to_weight = {'0': w[0], '1': w[1], '2':w[2]*10, '3': w[3]}\n",
    "weight_matrix = [convert_to_weight[str(i)] for i in y_train]\n",
    "\n",
    "Y[Y == 'Healthy'] = 0\n",
    "Y[Y == 'Disease-1'] = 1\n",
    "Y[Y == 'Disease-2'] = 2\n",
    "Y[Y == 'Disease-3'] = 3\n",
    "\n",
    "#create data matrix\n",
    "import xgboost as xgb\n",
    "data_matrix = xgb.DMatrix(data = X, label = Y)\n",
    "\n",
    "# create train and validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "#selected parameters\n",
    "params = {\n",
    "            'objective':'multi:softprob',\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators':200,\n",
    "            'n_threads' : 8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'alpha': 10\n",
    "        }\n",
    "           \n",
    "# initialize the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "\n",
    "#initialize data\n",
    "evaluation = [( X_train, y_train), ( X_val, y_val)]\n",
    "\n",
    "#fit the model\n",
    "xgb_clf.fit(X_train, y_train, eval_set=evaluation, eval_metric=\"mlogloss\", early_stopping_rounds=100, sample_weight = weight_matrix)\n",
    "\n",
    "#save model\n",
    "joblib.dump(xgb_clf, 'model_71_bacteria.pkl')\n",
    "\n",
    "#read test data and process\n",
    "df = pd.read_csv(\"Samplenormalized_test_data.csv\", index_col=False)\n",
    "X_test = df[diff_bacts]\n",
    "\n",
    "Y_test = df.iloc[:, 1096]\n",
    "\n",
    "Y_test[Y_test == 'Healthy'] = 0\n",
    "Y_test[Y_test == 'Disease-1'] = 1\n",
    "Y_test[Y_test == 'Disease-2'] = 2\n",
    "Y_test[Y_test == 'Disease-3'] = 3\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909376d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score  0.7650351725094421\n",
      "per classs f1 score  [0.81421865 0.79377432 0.359375   0.75949367]\n",
      "confusion matrix \n",
      " [[607  39  87  15]\n",
      " [ 45 306   3  36]\n",
      " [ 65   4  46   0]\n",
      " [ 26  32   5 180]]\n",
      "cohen cappa score  0.6345261867059373\n"
     ]
    }
   ],
   "source": [
    "print('weighted f1 score ', f1_score(list(Y_test), list(y_pred), average = 'weighted'))\n",
    "print('per classs f1 score ', f1_score(list(Y_test), list(y_pred), average = None))\n",
    "print('confusion matrix \\n', confusion_matrix(list(Y_test), list(y_pred)))\n",
    "print('cohen cappa score ', cohen_kappa_score(list(Y_test), list(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d3928",
   "metadata": {},
   "source": [
    "### we have been able to achieve comparable performance using only 71 bacteria using differentially enrichment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
